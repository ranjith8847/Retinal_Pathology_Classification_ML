{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras imports\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "# other imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "  \"model\"           : \"vgg16\",\n",
    "  \"weights\"         : \"imagenet\",\n",
    "\n",
    "  \"classifier_path\" : \"C:/Users/ranji/OneDrive/Desktop/CAP/OUTCOMES/classifier.pickle\",\n",
    "  \"img_p\" : \"C:/Users/ranji/OneDrive/Desktop/CAP/OCT2017/val/01-DME/dme8.jpeg\",\n",
    "    \"val\" : \"C:/Users/ranji/OneDrive/Desktop/CAP/OCT2017/val/validation\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading the classifier...\n"
     ]
    }
   ],
   "source": [
    "cur_path = config['val']\n",
    "# load the trained logistic regression classifier\n",
    "print (\"[INFO] loading the classifier...\")\n",
    "classifier = pickle.load(open(config[\"classifier_path\"], 'rb'))\n",
    "\n",
    "# pretrained models needed to perform feature extraction on test data too!\n",
    "if config[\"model\"] == \"vgg16\":\n",
    "\tbase_model = VGG16(weights=config[\"weights\"])\n",
    "\tmodel = Model(base_model.input, base_model.get_layer('fc1').output)\n",
    "\timage_size = (224, 224)\n",
    "\n",
    "else:\n",
    "\tbase_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 387ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 119ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 110ms/step\n",
      "1/1 [==============================] - 0s 114ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 107ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 122ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 111ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "Accuracy: 0.96875\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "\n",
    "\n",
    "# Define the path to the validation dataset folder\n",
    "validation_folder_path = 'C:/Users/ranji/OneDrive/Desktop/CAP/OCT2017/val'\n",
    "\n",
    "# Initialize variables for keeping track of the number of correct and incorrect predictions\n",
    "num_correct_predictions = 0\n",
    "num_incorrect_predictions = 0\n",
    "\n",
    "for image_path in glob.glob(cur_path + \"/*.JPEG\"):\n",
    "    img = keras.utils.load_img(image_path, target_size=image_size)\n",
    "\n",
    "    x = keras.utils.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    feature = model.predict(x)\n",
    "    flat = feature.flatten()\n",
    "    flat = np.expand_dims(flat, axis=0)\n",
    "    prediction = classifier.predict(flat)[0]\n",
    "    \n",
    "    ground_truth_label1 = image_path.split('_')[0]\n",
    "    ground_truth_label = int(ground_truth_label1[-1])\n",
    "\n",
    "    # Update the count of correct and incorrect predictions based on whether the predicted label matches the ground truth label\n",
    "    if prediction == ground_truth_label:\n",
    "        num_correct_predictions += 1\n",
    "    else:\n",
    "        num_incorrect_predictions += 1\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = num_correct_predictions / (num_correct_predictions + num_incorrect_predictions)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 109ms/step\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "img_p = config[\"img_p\"]\n",
    "#read in the image, pre-proces the image and make predictions\n",
    "img = keras.utils.load_img(img_p, target_size=image_size)\n",
    "x = keras.utils.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "feature = model.predict(x)\n",
    "flat = feature.flatten()\n",
    "flat = np.expand_dims(flat, axis=0)\n",
    "preds = classifier.predict(flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_check1 = ['normal','cnv', 'dme', 'drusen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dme'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_check1[preds[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "if preds!=0:\n",
    "    import cv2\n",
    "\n",
    "    # Load the retinal OCT scan image\n",
    "    img = cv2.imread(img_p)\n",
    "\n",
    "    # Get the dimensions of the image\n",
    "    height, width, channels = img.shape\n",
    "\n",
    "    # Define the region of interest (ROI) as the central 50% of the image\n",
    "    roi_x = int(width * 0.25)\n",
    "    roi_y = int(height * 0.25)\n",
    "    roi_width = int(width * 0.5)\n",
    "    roi_height = int(height * 0.5)\n",
    "\n",
    "    # Crop the image to the ROI\n",
    "    img_roi = img[roi_y:roi_y+roi_height, roi_x:roi_x+roi_width]\n",
    "\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(img_roi, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply thresholding to create a binary image with bright or dark spots\n",
    "    _, binary = cv2.threshold(gray, 30, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours of the binary image\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Find the contour with the largest area\n",
    "    largest_contour = None\n",
    "    largest_contour_area = 0\n",
    "    for contour in contours:\n",
    "        contour_area = cv2.contourArea(contour)\n",
    "        if contour_area > largest_contour_area:\n",
    "            largest_contour = contour\n",
    "            largest_contour_area = contour_area\n",
    "\n",
    "    # Draw the largest contour on the original image with green color and thickness of 2\n",
    "    if largest_contour is not None:\n",
    "        # Shift the contour to account for the ROI crop\n",
    "        largest_contour[:, :, 0] += roi_x\n",
    "        largest_contour[:, :, 1] += roi_y\n",
    "    \n",
    "        cv2.drawContours(img, [largest_contour], -1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the resulting image\n",
    "    cv2.imshow('Retinal OCT Scan with Highlighted Pathology', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "else:\n",
    "    img = cv2.imread(img_p)\n",
    "    cv2.imshow('Retinal OCT Scan with Highlighted Pathology', img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
